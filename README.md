## Классификация текстовых новостей. На основе текстового представления word2vec.

### Содержание:
1. Ноутбук с кодом **(nlp_text.ipynd)**
2. Русскоязычные обученные корпуса: 
    * news_upos_skipgram_300_5_2019; Русскоязычные новости; 2.6 миллиарда слов; *Universal Tags* **(184.zip)**
    * ruwikiruscorpora_upos_skipgram_300_2_2019; НКРЯ и Википедия за декабрь 2018; 788 миллионов слов; *Universal Tags* **(182.zip)**
3. Корпус для расстановки тегов (преобразование в « слово_его часть речи) **(udpipe_syntagrus.model)**
4. Текст рассказа *О. Генри "Русские соболя"* **(henry_soboly.txt)**
5. Таблица новостей с lenta.ru **(lenta-ru-news.csv)**
6. Таблица с предобработаными новостями **(good_text_teg.csv)**
7. Необходимые библиотеки **(requirements.txt)**


Датасеты >2Гб и русскоязычные корпуса w2v >600Мб распологаются на [Гугл-диске](https://drive.google.com/drive/folders/1q1dbX2PJzbIbgO0eKxap9dNHtkMdEgKO?usp=sharing)


### Описание алгоритма классификации.
#### Первоначальная задача
Изначально стояла задача, кластеризации текстовых новостей. 
Был предложен алгоритм: 
1. Предобработка с текстовыми новостями
    * Лемматизация 
    * Приведение всех слов к нижнему регистру 
    * Удаление стоп-слов 
    * Присвоение меток (тегов) (преобразование в «слово_его часть речи»)
2. Преобразование слов в вектора. На основе текстового представления word2vec.
Для данной задачи, мы выбираем уже обученную модель (с сайта https://rusvectores.org/ru/models/) . Модель обучена на русскоязычных новостях
3. Преобразование новостей в вектора.
Каждая новость – вектор. Преобразование происходит усреднением векторов из которых состоит новость.
4. Кластеризация векторов.
Выделение кластеров при помощи метода k-means (с помощью библиотеки ntk, для кластеризации векторов). Поскольку меру семантической близости  обычно измеряют с помощью косинусного расстояния, необходимо использовать косинусное расстояние, вместо Евклидового расстояния.
5. Выбор количества кластеров.
Выбор количества кластеров, путём максимизирования метрики качества кластеризации.
[**Силуэт**](https://ru.qaz.wiki/wiki/Silhouette_(clustering)#:~:text=%D0%A1%D0%B8%D0%BB%D1%83%D1%8D%D1%82%20%D0%BE%D1%82%D0%BD%D0%BE%D1%81%D0%B8%D1%82%D1%81%D1%8F%20%D0%BA%20%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D1%83%20%D0%B8%D0%BD%D1%82%D0%B5%D1%80%D0%BF%D1%80%D0%B5%D1%82%D0%B0%D1%86%D0%B8%D0%B8%20%D0%B8%20%D0%BF%D1%80%D0%BE%D0%B2%D0%B5%D1%80%D0%BA%D0%B8%20%D1%81%D0%BE%D0%B3%D0%BB%D0%B0%D1%81%D0%BE%D0%B2%D0%B0%D0%BD%D0%BD%D0%BE%D1%81%D1%82%D0%B8%20%D0%B2%20%D0%BF%D1%80%D0%B5%D0%B4%D0%B5%D0%BB%D0%B0%D1%85%20%D0%BA%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%BE%D0%B2%20%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85%20.&text=%D0%97%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%20%D1%81%D0%B8%D0%BB%D1%83%D1%8D%D1%82%D0%B0%20%D1%8F%D0%B2%D0%BB%D1%8F%D0%B5%D1%82%D1%81%D1%8F%20%D0%BC%D0%B5%D1%80%D0%BE%D0%B9%20%D1%82%D0%BE%D0%B3%D0%BE,%D1%81%20%D0%B4%D1%80%D1%83%D0%B3%D0%B8%D0%BC%D0%B8%20%D0%BA%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B0%D0%BC%D0%B8%20(%D1%80%D0%B0%D0%B7%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5).)
С помощью силуэта можно выбирать оптимальное число кластеров (если оно заранее не известно).
6. Выбор названия кластеров.
Из новостей, которые попали в данный кластер будут отобраны 5 слов с наибольшей частотой встречаемости.
Название кластера – перечисление 5 наиболее встречаемых слов, через запятую.
7. Оценка методом человеческого восприятия.
Для начала посмотреть, схожесть слов в теме кластера. Все 5 слов должны быть связаны между собой. Если имеются не связанные слова, то необходимо посмотреть на новости, которые попали в данный кластер. Выяснить, почему это произошло? Путём настройки гиперпараметров добиться наилучшего результата.

#### Задача классификации новостей.
Далее, задача была переформулирована в задачу классификации текстовых новостей. Было предложено реализовать данный алгоритм на размеченных данных.
(Предполагаю, что задачу кластеризации можно свести к задаче с частичным обучением с предварительным определением конечного числа кластеров(классов) и первоначальной разметкой данных)

##### Алгоритм классификации
Алгоритм классификации до третьего пункта аналогичен. 

&nbsp;4. Разбиение первоначальных данных на "обучение" и "тест"

&nbsp;5. Классификация текстовых новостей методом «ближайшего соседа» (KNN) используя косинусне расстояние.

&nbsp;6. Построения сетки и подбор оптимального значения числа ближайших соседей.

&nbsp;7. Построение модели классификации на основе метода опорных векторов (SVM).

&nbsp;8. Построения сетки и подбор оптимальных значений параметров модели: тип ядра и параметр регуляризации *С*.

##### Примечание: В ходе выполнения работы использовались варианты многопоточного расчёта и обучения, а также бэггинга для повышения точности модели и снижения времени обучения.
